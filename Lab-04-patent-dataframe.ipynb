{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 4253 / 5253 - Lab #4 - Patent Problem with Spark DataFrames\n",
    "<div>\n",
    " <h2> CSCI 4283 / 5253 \n",
    "  <IMG SRC=\"https://www.colorado.edu/cs/profiles/express/themes/cuspirit/logo.png\" WIDTH=50 ALIGN=\"right\"/> </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [Spark cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf) is useful as is [this reference on doing joins in Spark dataframe](http://www.learnbymarketing.com/1100/pyspark-joins-by-example/).\n",
    "\n",
    "The [DataBricks company has one of the better reference manuals for PySpark](https://docs.databricks.com/spark/latest/dataframes-datasets/index.html) -- they show you how to perform numerous common data operations such as joins, aggregation operations following `groupBy` and the like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following aggregation functions may be useful -- [these can be used to aggregate results of `groupby` operations](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html#example-aggregations-using-agg-and-countdistinct). More documentation is at the [PySpark SQL Functions manual](https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html#module-pyspark.sql.functions). Feel free to use other functions from that library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, countDistinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our session as described in the tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Lab4-Dataframe\") \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the citations and patents data and check that the data makes sense. Note that unlike in the RDD solution, the data is automatically inferred to be Integer() types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = spark.read.load('cite75_99.txt.gz',\n",
    "            format=\"csv\", sep=\",\", header=True,\n",
    "            compression=\"gzip\",\n",
    "            inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "| CITING|  CITED|\n",
      "+-------+-------+\n",
      "|3858241| 956203|\n",
      "|3858241|1324234|\n",
      "|3858241|3398406|\n",
      "|3858241|3557384|\n",
      "|3858241|3634889|\n",
      "+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "citations.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents = spark.read.load('apat63_99.txt.gz',\n",
    "            format=\"csv\", sep=\",\", header=True,\n",
    "            compression=\"gzip\",\n",
    "            inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+---+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+\n",
      "| PATENT|GYEAR|GDATE|APPYEAR|COUNTRY|POSTATE|ASSIGNEE|ASSCODE|CLAIMS|NCLASS|CAT|SUBCAT|CMADE|CRECEIVE|RATIOCIT|GENERAL|ORIGINAL|FWDAPLAG|BCKGTLAG|SELFCTUB|SELFCTLB|SECDUPBD|SECDLWBD|\n",
      "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+---+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|3070801| 1963| 1096|   NULL|     BE|   NULL|    NULL|      1|  NULL|   269|  6|    69| NULL|       1|    NULL|    0.0|    NULL|    NULL|    NULL|    NULL|    NULL|    NULL|    NULL|\n",
      "|3070802| 1963| 1096|   NULL|     US|     TX|    NULL|      1|  NULL|     2|  6|    63| NULL|       0|    NULL|   NULL|    NULL|    NULL|    NULL|    NULL|    NULL|    NULL|    NULL|\n",
      "|3070803| 1963| 1096|   NULL|     US|     IL|    NULL|      1|  NULL|     2|  6|    63| NULL|       9|    NULL| 0.3704|    NULL|    NULL|    NULL|    NULL|    NULL|    NULL|    NULL|\n",
      "|3070804| 1963| 1096|   NULL|     US|     OH|    NULL|      1|  NULL|     2|  6|    63| NULL|       3|    NULL| 0.6667|    NULL|    NULL|    NULL|    NULL|    NULL|    NULL|    NULL|\n",
      "|3070805| 1963| 1096|   NULL|     US|     CA|    NULL|      1|  NULL|     2|  6|    63| NULL|       1|    NULL|    0.0|    NULL|    NULL|    NULL|    NULL|    NULL|    NULL|    NULL|\n",
      "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+---+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patents.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_result DataFrame created successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce, lit, col, count\n",
    "patents_clean = patents.select(\n",
    "    col(\"PATENT\"), col(\"GYEAR\"), col(\"GDATE\"), col(\"APPYEAR\"), \n",
    "    col(\"COUNTRY\"), col(\"POSTATE\"), col(\"ASSIGNEE\"), col(\"ASSCODE\"),\n",
    "    col(\"CLAIMS\"), col(\"NCLASS\"), col(\"CAT\"), col(\"SUBCAT\"), \n",
    "    col(\"CMADE\"), col(\"CRECEIVE\"), col(\"RATIOCIT\"), col(\"GENERAL\"),\n",
    "    col(\"ORIGINAL\"), col(\"FWDAPLAG\"), col(\"BCKGTLAG\"), col(\"SELFCTUB\"),\n",
    "    col(\"SELFCTLB\"), col(\"SECDUPBD\"), col(\"SECDLWBD\")\n",
    ").filter(\n",
    "    (col(\"COUNTRY\") == \"US\") & \n",
    "    (col(\"POSTATE\").isNotNull()) & \n",
    "    (col(\"POSTATE\") != \"\")\n",
    ")\n",
    "\n",
    "#Creating patent-state lookup\n",
    "patent_states = patents_clean.select(\"PATENT\", \"POSTATE\").cache()\n",
    "\n",
    "# Joining citations with citing patent states\n",
    "citations_with_citing_state = citations.alias(\"c\") \\\n",
    "    .join(patent_states.alias(\"p1\"), col(\"c.CITING\") == col(\"p1.PATENT\"), \"inner\") \\\n",
    "    .select(\n",
    "        col(\"c.CITING\"),\n",
    "        col(\"c.CITED\"),\n",
    "        col(\"p1.POSTATE\").alias(\"CITING_STATE\")\n",
    "    )\n",
    "\n",
    "# Joining with cited patent states\n",
    "citations_with_both_states = citations_with_citing_state.alias(\"ccs\") \\\n",
    "    .join(patent_states.alias(\"p2\"), col(\"ccs.CITED\") == col(\"p2.PATENT\"), \"inner\") \\\n",
    "    .select(\n",
    "        col(\"ccs.CITING\"),\n",
    "        col(\"ccs.CITED\"),\n",
    "        col(\"ccs.CITING_STATE\"),\n",
    "        col(\"p2.POSTATE\").alias(\"CITED_STATE\")\n",
    "    )\n",
    "\n",
    "# Filtering for same-state citations and count\n",
    "same_state_citations = citations_with_both_states \\\n",
    "    .filter(col(\"CITING_STATE\") == col(\"CITED_STATE\")) \\\n",
    "    .groupBy(\"CITING\") \\\n",
    "    .agg(count(\"*\").alias(\"SAME_STATE\"))\n",
    "\n",
    "# Creating final_result DataFrame (THIS IS WHAT WAS MISSING!)\n",
    "final_result = patents_clean.alias(\"p\") \\\n",
    "    .join(same_state_citations.alias(\"s\"), col(\"p.PATENT\") == col(\"s.CITING\"), \"left\") \\\n",
    "    .select(\n",
    "        col(\"p.*\"),\n",
    "        coalesce(col(\"s.SAME_STATE\"), lit(0)).alias(\"SAME_STATE\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"SAME_STATE\").desc(), col(\"PATENT\").asc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "print(\"final_result DataFrame created successfully!\")\n",
    "\n",
    "results = final_result.collect()\n",
    "headers = ['PATENT', 'GYEAR', 'GDATE', 'APPYEAR', 'COUNTRY', 'POSTATE', 'ASSIGNEE', 'ASSCODE', \n",
    "           'CLAIMS', 'NCLASS', 'CAT', 'SUBCAT', 'CMADE', 'CRECEIVE', 'RATIOCIT', 'GENERAL', \n",
    "           'ORIGINAL', 'FWDAPLAG', 'BCKGTLAG', 'SELFCTUB', 'SELFCTLB', 'SECDUPBD', 'SECDLWBD', 'SAME_STATE']\n",
    "col_widths = [8, 6, 8, 8, 8, 8, 12, 8, 7, 7, 5, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11]\n",
    "\n",
    "# Printing header\n",
    "header_line = \"\"\n",
    "for i, header in enumerate(headers):\n",
    "    header_line += f\"{header:<{col_widths[i]}}\"\n",
    "print(header_line)\n",
    "print(\"-\" * builtins.sum(col_widths))\n",
    "\n",
    "# Print data rows\n",
    "for row in results:\n",
    "    assignee_str = str(row.ASSIGNEE) if row.ASSIGNEE is not None else 'null'\n",
    "    assignee_short = assignee_str[:10] if len(assignee_str) > 10 else assignee_str\n",
    "    \n",
    "    values = [\n",
    "        row.PATENT, row.GYEAR, row.GDATE, row.APPYEAR, row.COUNTRY, row.POSTATE,\n",
    "        assignee_short, row.ASSCODE, row.CLAIMS, row.NCLASS, row.CAT, row.SUBCAT, \n",
    "        row.CMADE, row.CRECEIVE, \n",
    "        f\"{row.RATIOCIT:.4f}\" if row.RATIOCIT is not None else 'null',\n",
    "        f\"{row.GENERAL:.4f}\" if row.GENERAL is not None else 'null',\n",
    "        f\"{row.ORIGINAL:.4f}\" if row.ORIGINAL is not None else 'null',\n",
    "        f\"{row.FWDAPLAG:.4f}\" if row.FWDAPLAG is not None else 'null',\n",
    "        f\"{row.BCKGTLAG:.4f}\" if row.BCKGTLAG is not None else 'null',\n",
    "        f\"{row.SELFCTUB:.4f}\" if row.SELFCTUB is not None else 'null',\n",
    "        f\"{row.SELFCTLB:.4f}\" if row.SELFCTLB is not None else 'null',\n",
    "        f\"{row.SECDUPBD:.4f}\" if row.SECDUPBD is not None else 'null',\n",
    "        f\"{row.SECDLWBD:.4f}\" if row.SECDLWBD is not None else 'null',\n",
    "        row.SAME_STATE\n",
    "    ]\n",
    "    \n",
    "    row_line = \"\"\n",
    "    for i, value in enumerate(values):\n",
    "        str_val = str(value) if value is not None else 'null'\n",
    "        if len(str_val) > col_widths[i] - 1:\n",
    "            str_val = str_val[:col_widths[i] - 3] + \"..\"\n",
    "        row_line += f\"{str_val:<{col_widths[i]}}\"\n",
    "    print(row_line)\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
